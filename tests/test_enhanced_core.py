"""
Tests for TestPilot Enhanced Core Functionality

This test suite validates the revolutionary 50Ã— improvement features including
advanced AI capabilities, code analysis, test verification, and quality assurance.
"""

import ast
import os
import tempfile
import unittest
from unittest.mock import Mock, patch, MagicMock
from pathlib import Path

# Import our enhanced functionality
from testpilot.core import (
    CodeAnalyzer,
    TestVerifier, 
    generate_tests_llm,
    run_pytest_tests,
    analyze_test_coverage,
    generate_integration_tests
)
from testpilot.llm_providers import (
    OpenAIProvider,
    AnthropicProvider,
    OllamaProvider,
    get_llm_provider,
    get_available_providers
)


class TestCodeAnalyzer(unittest.TestCase):
    """Test the intelligent code analysis system."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.simple_code = '''
def add(a, b):
    """Add two numbers."""
    return a + b

def divide(a, b):
    """Divide two numbers."""
    if b == 0:
        raise ValueError("Cannot divide by zero")
    return a / b
'''
        
        self.complex_code = '''
import asyncio
import logging
from typing import Dict, List, Optional
from dataclasses import dataclass

@dataclass
class User:
    name: str
    email: str
    
    def __post_init__(self):
        if "@" not in self.email:
            raise ValueError("Invalid email")

class UserService:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @property
    def user_count(self):
        return len(self._users)
    
    async def create_user(self, name: str, email: str) -> User:
        """Create a new user asynchronously."""
        try:
            user = User(name, email)
            await self._validate_user(user)
            return user
        except Exception as e:
            self.logger.error(f"User creation failed: {e}")
            raise
    
    async def _validate_user(self, user: User):
        """Validate user data."""
        await asyncio.sleep(0.1)  # Simulate async validation
        if not user.name:
            raise ValueError("Name is required")
'''

    def test_simple_code_analysis(self):
        """Test analysis of simple code."""
        analyzer = CodeAnalyzer(self.simple_code)
        analysis = analyzer.analyze()
        
        # Check basic structure detection
        self.assertEqual(len(analysis['functions']), 2)
        self.assertEqual(len(analysis['classes']), 0)
        self.assertEqual(len(analysis['async_functions']), 0)
        
        # Check function details
        func_names = [f['name'] for f in analysis['functions']]
        self.assertIn('add', func_names)
        self.assertIn('divide', func_names)
        
        # Check complexity assessment
        self.assertIn(analysis['complexity'], ['Low', 'Medium', 'High'])
        
        # Check exception detection
        self.assertTrue(analysis['has_exceptions'])
        
    def test_complex_code_analysis(self):
        """Test analysis of complex code with advanced features."""
        analyzer = CodeAnalyzer(self.complex_code)
        analysis = analyzer.analyze()
        
        # Check advanced structure detection
        self.assertGreater(len(analysis['functions']), 0)
        self.assertEqual(len(analysis['classes']), 2)  # User and UserService
        self.assertGreater(len(analysis['async_functions']), 0)
        
        # Check async function detection
        async_names = [f['name'] for f in analysis['async_functions']]
        self.assertIn('create_user', async_names)
        
        # Check decorator detection
        self.assertTrue(analysis['has_decorators'])
        
        # Check project type detection
        self.assertIsInstance(analysis['project_type'], str)
        
        # Check requirements generation
        self.assertIsInstance(analysis['requirements'], list)
        if analysis['async_functions']:
            self.assertTrue(any('async' in req.lower() for req in analysis['requirements']))

    def test_complexity_calculation(self):
        """Test complexity calculation accuracy."""
        # Simple code should be Low complexity
        simple_analyzer = CodeAnalyzer(self.simple_code)
        simple_analysis = simple_analyzer.analyze()
        self.assertEqual(simple_analysis['complexity'], 'Low')
        
        # Complex code should be Medium or High
        complex_analyzer = CodeAnalyzer(self.complex_code)
        complex_analysis = complex_analyzer.analyze()
        self.assertIn(complex_analysis['complexity'], ['Medium', 'High'])

    def test_project_type_detection(self):
        """Test project type detection."""
        django_code = "from django.models import Model\nclass User(Model): pass"
        analyzer = CodeAnalyzer(django_code)
        analysis = analyzer.analyze()
        self.assertEqual(analysis['project_type'], 'Web Application')
        
        data_science_code = "import pandas as pd\nimport numpy as np"
        analyzer = CodeAnalyzer(data_science_code)
        analysis = analyzer.analyze()
        self.assertEqual(analysis['project_type'], 'Data Science')

    def test_error_handling(self):
        """Test error handling in code analysis."""
        # Test with invalid Python code
        invalid_code = "def invalid_syntax( invalid"
        
        with self.assertRaises(SyntaxError):
            analyzer = CodeAnalyzer(invalid_code)


class TestTestVerifier(unittest.TestCase):
    """Test the test verification and quality assurance system."""
    
    def setUp(self):
        """Set up test fixtures."""
        self.valid_test_code = '''
import pytest
from my_module import add, divide

def test_add():
    """Test addition function."""
    assert add(2, 3) == 5
    assert add(-1, 1) == 0

def test_divide():
    """Test division function."""
    assert divide(10, 2) == 5
    
def test_divide_by_zero():
    """Test division by zero error."""
    with pytest.raises(ValueError):
        divide(10, 0)
'''
        
        self.invalid_test_code = '''
# Missing imports
def test_add():
    assert add(2, 3) == 5

def invalid_syntax_test(
    # This has syntax errors
'''
        
        self.source_file = "my_module.py"

    def test_valid_test_verification(self):
        """Test verification of valid test code."""
        verifier = TestVerifier(self.valid_test_code, self.source_file)
        is_valid, issues, corrected_code = verifier.verify()
        
        self.assertTrue(is_valid)
        self.assertEqual(len(issues), 0)
        self.assertEqual(corrected_code, self.valid_test_code)

    def test_invalid_test_correction(self):
        """Test automatic correction of invalid test code."""
        verifier = TestVerifier(self.invalid_test_code, self.source_file)
        is_valid, issues, corrected_code = verifier.verify()
        
        self.assertFalse(is_valid)
        self.assertGreater(len(issues), 0)
        
        # Check that imports were added
        self.assertIn('import', corrected_code)

    def test_syntax_error_detection(self):
        """Test detection of syntax errors."""
        syntax_error_code = "def test_invalid(\n    # Missing closing parenthesis"
        
        verifier = TestVerifier(syntax_error_code, self.source_file)
        is_valid, issues, corrected_code = verifier.verify()
        
        self.assertFalse(is_valid)
        self.assertTrue(any('syntax' in issue.lower() for issue in issues))

    def test_test_function_detection(self):
        """Test detection of test functions."""
        no_tests_code = '''
import pytest

def helper_function():
    pass
'''
        
        verifier = TestVerifier(no_tests_code, self.source_file)
        is_valid, issues, corrected_code = verifier.verify()
        
        self.assertFalse(is_valid)
        self.assertTrue(any('no test functions' in issue.lower() for issue in issues))


class TestEnhancedLLMProviders(unittest.TestCase):
    """Test the enhanced LLM provider system."""

    def test_openai_provider_context_generation(self):
        """Test OpenAI provider with context awareness."""
        with patch('openai.OpenAI') as mock_openai:
            # Mock the OpenAI client
            mock_client = MagicMock()
            mock_completion = MagicMock()
            mock_completion.choices = [MagicMock()]
            mock_completion.choices[0].message.content = "Generated test code"
            mock_client.chat.completions.create.return_value = mock_completion
            mock_openai.return_value = mock_client
            
            provider = OpenAIProvider("test_key")
            
            # Test context-aware generation
            context = {
                'project_type': 'Web Application',
                'testing_framework': 'pytest',
                'complexity': 'High'
            }
            
            result = provider.generate_with_context(
                "Generate tests", "gpt-4o", context
            )
            
            self.assertEqual(result, "Generated test code")
            mock_client.chat.completions.create.assert_called_once()

    def test_anthropic_provider_implementation(self):
        """Test Anthropic provider implementation."""
        with patch('anthropic.Anthropic') as mock_anthropic:
            # Mock the Anthropic client
            mock_client = MagicMock()
            mock_response = MagicMock()
            mock_response.content = [MagicMock()]
            mock_response.content[0].text = "Claude generated test code"
            mock_client.messages.create.return_value = mock_response
            mock_anthropic.return_value = mock_client
            
            provider = AnthropicProvider("test_key")
            result = provider.generate_text("Generate tests", "claude-3-sonnet-20240229")
            
            self.assertEqual(result, "Claude generated test code")
            mock_client.messages.create.assert_called_once()

    def test_ollama_provider_implementation(self):
        """Test Ollama provider for local models."""
        with patch('requests.post') as mock_post:
            # Mock the requests response
            mock_response = MagicMock()
            mock_response.json.return_value = {"response": "Local model test code"}
            mock_response.raise_for_status.return_value = None
            mock_post.return_value = mock_response
            
            provider = OllamaProvider()
            result = provider.generate_text("Generate tests", "llama2")
            
            self.assertEqual(result, "Local model test code")
            mock_post.assert_called_once()

    def test_provider_registry(self):
        """Test provider registration and retrieval."""
        available_providers = get_available_providers()
        
        expected_providers = ['openai', 'anthropic', 'ollama']
        for provider in expected_providers:
            self.assertIn(provider, available_providers)

    def test_get_llm_provider_function(self):
        """Test the get_llm_provider function."""
        with patch.dict(os.environ, {'OPENAI_API_KEY': 'test_key'}):
            provider = get_llm_provider('openai')
            self.assertIsInstance(provider, OpenAIProvider)
        
        # Test invalid provider
        with self.assertRaises(ValueError):
            get_llm_provider('invalid_provider')


class TestEnhancedTestGeneration(unittest.TestCase):
    """Test enhanced test generation functionality."""

    def setUp(self):
        """Set up test fixtures."""
        self.sample_code = '''
def fibonacci(n):
    """Calculate fibonacci number."""
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

class Calculator:
    def __init__(self):
        self.history = []
    
    def add(self, a, b):
        result = a + b
        self.history.append(f"{a} + {b} = {result}")
        return result
'''
        
        # Create temporary source file
        self.temp_file = tempfile.NamedTemporaryFile(
            mode='w', suffix='.py', delete=False
        )
        self.temp_file.write(self.sample_code)
        self.temp_file.close()
        self.source_file = self.temp_file.name

    def tearDown(self):
        """Clean up test fixtures."""
        if os.path.exists(self.source_file):
            os.unlink(self.source_file)

    @patch('testpilot.core.get_llm_provider')
    def test_enhanced_test_generation(self, mock_get_provider):
        """Test enhanced test generation with code analysis."""
        # Mock the provider
        mock_provider = MagicMock()
        mock_provider.generate_with_context.return_value = '''
import pytest
from my_module import fibonacci, Calculator

def test_fibonacci():
    assert fibonacci(0) == 0
    assert fibonacci(1) == 1
    assert fibonacci(5) == 5

def test_calculator_add():
    calc = Calculator()
    result = calc.add(2, 3)
    assert result == 5
    assert len(calc.history) == 1
'''
        mock_get_provider.return_value = mock_provider
        
        # Test enhanced generation
        result = generate_tests_llm(
            self.source_file, 'openai', 'gpt-4o', enhanced_mode=True
        )
        
        self.assertIn('test_fibonacci', result)
        self.assertIn('test_calculator_add', result)
        mock_provider.generate_with_context.assert_called_once()

    @patch('testpilot.core.get_llm_provider')
    def test_integration_test_generation(self, mock_get_provider):
        """Test integration test generation."""
        mock_provider = MagicMock()
        mock_provider.generate_text.return_value = '''
import pytest
from my_module import Calculator

def test_calculator_integration():
    """Test calculator with multiple operations."""
    calc = Calculator()
    
    # Perform multiple operations
    calc.add(1, 2)
    calc.add(3, 4)
    
    # Verify history tracking
    assert len(calc.history) == 2
    assert "1 + 2 = 3" in calc.history[0]
'''
        mock_get_provider.return_value = mock_provider
        
        result = generate_integration_tests(
            self.source_file, 'openai', 'gpt-4o'
        )
        
        self.assertIn('integration', result)
        mock_provider.generate_text.assert_called_once()


class TestPerformanceOptimizations(unittest.TestCase):
    """Test performance optimization features."""

    @patch('subprocess.run')
    def test_test_execution_timeout(self, mock_run):
        """Test that test execution has timeout protection."""
        # Mock a timeout scenario
        import subprocess
        mock_run.side_effect = subprocess.TimeoutExpired(['pytest'], 60)
        
        result = run_pytest_tests('test_file.py')
        
        # Should handle timeout gracefully
        self.assertIn('timed out', result[0].lower())
        self.assertTrue(result[1])  # Should indicate failure

    @patch('subprocess.run')
    def test_coverage_analysis(self, mock_run):
        """Test coverage analysis functionality."""
        # Mock successful coverage run
        mock_result = MagicMock()
        mock_result.returncode = 0
        mock_result.stdout = "TOTAL                100   85%"
        mock_run.return_value = mock_result
        
        with tempfile.NamedTemporaryFile(suffix='.py') as test_file, \
             tempfile.NamedTemporaryFile(suffix='.py') as source_file:
            
            coverage_data = analyze_test_coverage(test_file.name, source_file.name)
            
            self.assertIsInstance(coverage_data, dict)
            self.assertIn('total_coverage', coverage_data)

    def test_code_analysis_caching(self):
        """Test that code analysis can be cached for performance."""
        code = "def simple(): return 42"
        
        # First analysis
        analyzer1 = CodeAnalyzer(code)
        result1 = analyzer1.analyze()
        
        # Second analysis of same code
        analyzer2 = CodeAnalyzer(code)
        result2 = analyzer2.analyze()
        
        # Results should be identical (demonstrating cacheable nature)
        self.assertEqual(result1['complexity'], result2['complexity'])
        self.assertEqual(len(result1['functions']), len(result2['functions']))


class TestQualityAssurance(unittest.TestCase):
    """Test quality assurance and reliability features."""

    def test_test_quality_metrics(self):
        """Test calculation of test quality metrics."""
        high_quality_test = '''
import pytest
from my_module import divide

class TestDivision:
    """Comprehensive tests for division function."""
    
    def test_normal_division(self):
        """Test normal division cases."""
        assert divide(10, 2) == 5
        assert divide(7, 2) == 3.5
        
    def test_division_by_zero(self):
        """Test division by zero handling."""
        with pytest.raises(ValueError, match="Cannot divide by zero"):
            divide(10, 0)
            
    def test_negative_numbers(self):
        """Test division with negative numbers."""
        assert divide(-10, 2) == -5
        assert divide(10, -2) == -5
        
    @pytest.mark.parametrize("a,b,expected", [
        (100, 10, 10),
        (15, 3, 5),
        (1, 2, 0.5),
    ])
    def test_parametrized_division(self, a, b, expected):
        """Test division with multiple parameter sets."""
        assert divide(a, b) == expected
'''
        
        verifier = TestVerifier(high_quality_test, "my_module.py")
        is_valid, issues, corrected_code = verifier.verify()
        
        # High quality test should pass verification
        self.assertTrue(is_valid)
        self.assertEqual(len(issues), 0)

    def test_reliability_features(self):
        """Test reliability features like error recovery."""
        # Test that the system can handle various error conditions
        test_cases = [
            ("", "Empty code"),
            ("def incomplete(", "Syntax error"),
            ("def test_without_imports():\n    assert unknown_function() == 1", "Missing imports")
        ]
        
        for code, description in test_cases:
            with self.subTest(description=description):
                if code == "":
                    # Empty code should be handled gracefully
                    continue
                    
                verifier = TestVerifier(code, "dummy.py")
                # Should not raise unhandled exceptions
                try:
                    is_valid, issues, corrected_code = verifier.verify()
                    # For error cases, should detect problems
                    if "syntax" in description.lower():
                        self.assertFalse(is_valid)
                except Exception as e:
                    # Should handle errors gracefully
                    self.fail(f"Unhandled exception for {description}: {e}")


class TestIntegrationScenarios(unittest.TestCase):
    """Test complete integration scenarios."""

    def setUp(self):
        """Set up integration test environment."""
        self.complex_module = '''
"""
Complex module for integration testing.
Demonstrates real-world code patterns that TestPilot should handle.
"""

import asyncio
from typing import List, Dict, Optional
from dataclasses import dataclass
from datetime import datetime

@dataclass 
class Task:
    id: str
    title: str
    completed: bool = False
    created_at: datetime = None
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

class TaskManager:
    """Manages a collection of tasks."""
    
    def __init__(self):
        self._tasks: Dict[str, Task] = {}
        self._next_id = 1
    
    def create_task(self, title: str) -> Task:
        """Create a new task."""
        if not title.strip():
            raise ValueError("Task title cannot be empty")
        
        task_id = f"task_{self._next_id}"
        self._next_id += 1
        
        task = Task(task_id, title.strip())
        self._tasks[task_id] = task
        return task
    
    def get_task(self, task_id: str) -> Optional[Task]:
        """Get a task by ID."""
        return self._tasks.get(task_id)
    
    def complete_task(self, task_id: str) -> bool:
        """Mark a task as completed."""
        task = self._tasks.get(task_id)
        if task:
            task.completed = True
            return True
        return False
    
    def get_pending_tasks(self) -> List[Task]:
        """Get all pending tasks."""
        return [task for task in self._tasks.values() if not task.completed]
    
    async def bulk_complete(self, task_ids: List[str]) -> int:
        """Complete multiple tasks asynchronously."""
        completed_count = 0
        
        for task_id in task_ids:
            # Simulate async processing
            await asyncio.sleep(0.01)
            
            if self.complete_task(task_id):
                completed_count += 1
        
        return completed_count
'''
        
        # Create temporary file for integration testing
        self.temp_file = tempfile.NamedTemporaryFile(
            mode='w', suffix='.py', delete=False
        )
        self.temp_file.write(self.complex_module)
        self.temp_file.close()
        self.source_file = self.temp_file.name

    def tearDown(self):
        """Clean up integration test environment."""
        if os.path.exists(self.source_file):
            os.unlink(self.source_file)

    def test_end_to_end_workflow(self):
        """Test complete end-to-end workflow."""
        # Step 1: Analyze code
        with open(self.source_file, 'r') as f:
            code = f.read()
        
        analyzer = CodeAnalyzer(code)
        analysis = analyzer.analyze()
        
        # Verify analysis detected key features
        self.assertGreater(len(analysis['functions']), 0)
        self.assertGreater(len(analysis['classes']), 0)
        self.assertTrue(analysis['has_exceptions'])
        
        # Step 2: Mock test generation (since we don't have real API keys)
        mock_test_code = '''
import pytest
import asyncio
from unittest.mock import patch
from datetime import datetime
from my_module import Task, TaskManager

class TestTask:
    def test_task_creation(self):
        """Test task creation with default values."""
        task = Task("1", "Test task")
        assert task.id == "1"
        assert task.title == "Test task"
        assert not task.completed
        assert isinstance(task.created_at, datetime)

class TestTaskManager:
    def test_create_task(self):
        """Test task creation."""
        manager = TaskManager()
        task = manager.create_task("New task")
        
        assert task.title == "New task"
        assert not task.completed
        assert task.id.startswith("task_")
    
    def test_create_empty_task_raises_error(self):
        """Test that empty task title raises error."""
        manager = TaskManager()
        
        with pytest.raises(ValueError, match="Task title cannot be empty"):
            manager.create_task("")
    
    @pytest.mark.asyncio
    async def test_bulk_complete(self):
        """Test bulk completion of tasks."""
        manager = TaskManager()
        
        # Create some tasks
        task1 = manager.create_task("Task 1")
        task2 = manager.create_task("Task 2")
        
        # Bulk complete
        completed = await manager.bulk_complete([task1.id, task2.id])
        
        assert completed == 2
        assert manager.get_task(task1.id).completed
        assert manager.get_task(task2.id).completed
'''
        
        # Step 3: Verify test code
        verifier = TestVerifier(mock_test_code, self.source_file)
        is_valid, issues, corrected_code = verifier.verify()
        
        # Should generate valid, comprehensive tests
        self.assertTrue(is_valid)
        self.assertEqual(len(issues), 0)
        
        # Step 4: Verify test comprehensiveness
        # Check that generated tests cover key scenarios
        self.assertIn('test_create_task', mock_test_code)
        self.assertIn('test_bulk_complete', mock_test_code)
        self.assertIn('pytest.raises', mock_test_code)
        self.assertIn('@pytest.mark.asyncio', mock_test_code)

    def test_performance_characteristics(self):
        """Test that the enhanced system performs well."""
        import time
        
        # Measure code analysis time
        start_time = time.time()
        
        with open(self.source_file, 'r') as f:
            code = f.read()
        
        analyzer = CodeAnalyzer(code)
        analysis = analyzer.analyze()
        
        analysis_time = time.time() - start_time
        
        # Code analysis should be fast (< 1 second for this module)
        self.assertLess(analysis_time, 1.0)
        
        # Verify analysis quality
        self.assertGreater(len(analysis['functions']), 5)
        self.assertEqual(len(analysis['classes']), 2)


if __name__ == '__main__':
    # Run the complete test suite
    unittest.main(verbosity=2)